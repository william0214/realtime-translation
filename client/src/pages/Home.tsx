import { Button } from "@/components/ui/button";
import {
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
} from "@/components/ui/select";
import { trpc } from "@/lib/trpc";
import { Download, Mic, Trash2 } from "lucide-react";
import { useCallback, useEffect, useRef, useState } from "react";
import { toast } from "sonner";

type ConversationMessage = {
  id: number;
  speaker: "nurse" | "patient";
  originalText: string;
  translatedText: string;
  detectedLanguage: string;
  timestamp: Date;
};

type TranslationResult = {
  success: boolean;
  direction?: "nurse_to_patient" | "patient_to_nurse";
  sourceLang?: string;
  targetLang?: string;
  sourceText?: string;
  translatedText?: string;
  error?: string;
};

type ProcessingStatus = "listening" | "recognizing" | "translating" | "idle";

const LANGUAGE_OPTIONS = [
  { value: "vi", label: "è¶Šå—èª" },
  { value: "id", label: "å°å°¼èª" },
  { value: "fil", label: "è²å¾‹è³“èª" },
  { value: "en", label: "è‹±æ–‡" },
  { value: "it", label: "ç¾©å¤§åˆ©èª" },
  { value: "ja", label: "æ—¥æ–‡" },
  { value: "ko", label: "éŸ“æ–‡" },
  { value: "th", label: "æ³°æ–‡" },
];

// VAD è¨­å®šï¼ˆä½¿ç”¨ RMS éŸ³é‡æª¢æ¸¬ï¼‰
const RMS_THRESHOLD = 0.02; // RMS é–¾å€¼
const SILENCE_FRAMES_THRESHOLD = 10; // ç´„ 800ms éœéŸ³åˆ¤å®šç‚ºèªéŸ³çµæŸ
const AUDIO_PROCESS_BUFFER_SIZE = 4096;

// WAV ç·¨ç¢¼å‡½æ•¸
function encodeWAV(samples: Float32Array[], sampleRate: number): Blob {
  const length = samples.reduce((acc, arr) => acc + arr.length, 0);
  const buffer = new ArrayBuffer(44 + length * 2);
  const view = new DataView(buffer);

  function writeString(view: DataView, offset: number, string: string) {
    for (let i = 0; i < string.length; i++) {
      view.setUint8(offset + i, string.charCodeAt(i));
    }
  }

  writeString(view, 0, "RIFF");
  view.setUint32(4, 36 + length * 2, true);
  writeString(view, 8, "WAVE");
  writeString(view, 12, "fmt ");
  view.setUint32(16, 16, true);
  view.setUint16(20, 1, true);
  view.setUint16(22, 1, true);
  view.setUint32(24, sampleRate, true);
  view.setUint32(28, sampleRate * 2, true);
  view.setUint16(32, 2, true);
  view.setUint16(34, 16, true);
  writeString(view, 36, "data");
  view.setUint32(40, length * 2, true);

  let offset = 44;
  let index = 0;
  for (const sample of samples) {
    for (let i = 0; i < sample.length; i++, offset += 2, index++) {
      const s = Math.max(-1, Math.min(1, sample[i]));
      view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7fff, true);
    }
  }

  return new Blob([view], { type: "audio/wav" });
}

// RMS éŸ³é‡æª¢æ¸¬
function isSpeaking(audioArray: Float32Array): boolean {
  let sum = 0;
  for (let i = 0; i < audioArray.length; i++) {
    sum += audioArray[i] * audioArray[i];
  }
  const rms = Math.sqrt(sum / audioArray.length);
  return rms > RMS_THRESHOLD;
}

export default function Home() {
  const [isRecording, setIsRecording] = useState(false);
  const [conversations, setConversations] = useState<ConversationMessage[]>([]);
  const [targetLanguage, setTargetLanguage] = useState<string>("vi");
  const [audioLevel, setAudioLevel] = useState<number>(0);
  const [processingStatus, setProcessingStatus] = useState<ProcessingStatus>("idle");

  const audioContextRef = useRef<AudioContext | null>(null);
  const processorRef = useRef<ScriptProcessorNode | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const messageIdRef = useRef(0);
  
  // Refs for auto-scroll
  const nurseScrollRef = useRef<HTMLDivElement>(null);
  const patientScrollRef = useRef<HTMLDivElement>(null);

  // VAD ç‹€æ…‹
  const currentSpeechBufferRef = useRef<Float32Array[]>([]);
  const speakingRef = useRef(false);
  const silenceFramesRef = useRef(0);

  const autoTranslateMutation = trpc.translation.autoTranslate.useMutation({
    onSuccess: (data: TranslationResult) => {
      if (data.success && data.sourceText && data.translatedText) {
        const speaker = data.direction === "nurse_to_patient" ? "nurse" : "patient";

        const newMessage: ConversationMessage = {
          id: messageIdRef.current++,
          speaker,
          originalText: data.sourceText,
          translatedText: data.translatedText,
          detectedLanguage: data.sourceLang || "unknown",
          timestamp: new Date(),
        };

        setConversations((prev) => [...prev, newMessage]);
        setProcessingStatus("listening");
        
        // Auto-scroll to latest message
        setTimeout(() => {
          if (speaker === "nurse" && nurseScrollRef.current) {
            nurseScrollRef.current.scrollTop = nurseScrollRef.current.scrollHeight;
          } else if (speaker === "patient" && patientScrollRef.current) {
            patientScrollRef.current.scrollTop = patientScrollRef.current.scrollHeight;
          }
        }, 100);
      } else if (data.error) {
        console.log("Translation error:", data.error);
        setProcessingStatus("listening");
      }
    },
    onError: (error) => {
      toast.error("ç¿»è­¯å¤±æ•—ï¼š" + error.message);
      setProcessingStatus("listening");
    },
  });

  const sendToWhisper = useCallback(
    (audioBlob: Blob) => {
      setProcessingStatus("recognizing");

      const reader = new FileReader();
      reader.onloadend = () => {
        const base64Audio = (reader.result as string).split(",")[1];
        if (base64Audio) {
          setProcessingStatus("translating");
          autoTranslateMutation.mutate({
            audioBase64: base64Audio,
            filename: `audio-${Date.now()}.wav`,
            preferredTargetLang: targetLanguage,
          });
        }
      };
      reader.readAsDataURL(audioBlob);
    },
    [autoTranslateMutation, targetLanguage]
  );

  const startRecording = useCallback(async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      streamRef.current = stream;

      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
      const source = audioContext.createMediaStreamSource(stream);
      const processor = audioContext.createScriptProcessor(AUDIO_PROCESS_BUFFER_SIZE, 1, 1);

      audioContextRef.current = audioContext;
      processorRef.current = processor;

      source.connect(processor);
      processor.connect(audioContext.destination);

      processor.onaudioprocess = (e) => {
        const data = e.inputBuffer.getChannelData(0);
        const dataCopy = new Float32Array(data);

        // è¨ˆç®— RMS ä¸¦æ›´æ–°éŸ³é‡é¡¯ç¤º
        let sum = 0;
        for (let i = 0; i < dataCopy.length; i++) {
          sum += dataCopy[i] * dataCopy[i];
        }
        const rms = Math.sqrt(sum / dataCopy.length);
        setAudioLevel(rms / RMS_THRESHOLD); // æ­£è¦åŒ–é¡¯ç¤º

        // VAD åˆ¤æ–·
        if (isSpeaking(dataCopy)) {
          speakingRef.current = true;
          silenceFramesRef.current = 0;
          currentSpeechBufferRef.current.push(dataCopy);
        } else {
          if (speakingRef.current) {
            silenceFramesRef.current++;
            if (silenceFramesRef.current > SILENCE_FRAMES_THRESHOLD) {
              // èªéŸ³çµæŸï¼Œç™¼é€åˆ° Whisper
              speakingRef.current = false;
              const wavBlob = encodeWAV(
                currentSpeechBufferRef.current,
                audioContext.sampleRate
              );
              
              // æª¢æŸ¥ WAV å¤§å°
              if (wavBlob.size > 1000) {
                sendToWhisper(wavBlob);
              } else {
                console.log("Speech too short, skipping...");
              }

              currentSpeechBufferRef.current = [];
              silenceFramesRef.current = 0;
            }
          }
        }
      };

      setIsRecording(true);
      setProcessingStatus("listening");
      toast.success("é–‹å§‹å°è©±");
    } catch (error) {
      toast.error("ç„¡æ³•å•Ÿå‹•éº¥å…‹é¢¨ï¼š" + (error as Error).message);
    }
  }, [sendToWhisper]);

  const stopRecording = useCallback(() => {
    if (processorRef.current) {
      processorRef.current.disconnect();
      processorRef.current = null;
    }

    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }

    if (streamRef.current) {
      streamRef.current.getTracks().forEach((track) => track.stop());
      streamRef.current = null;
    }

    currentSpeechBufferRef.current = [];
    speakingRef.current = false;
    silenceFramesRef.current = 0;

    setIsRecording(false);
    setAudioLevel(0);
    setProcessingStatus("idle");
    toast.success("çµæŸå°è©±");
  }, []);

  const clearConversations = useCallback(() => {
    setConversations([]);
    messageIdRef.current = 0;
    toast.success("å·²æ¸…é™¤å°è©±è¨˜éŒ„");
  }, []);

  const exportConversations = useCallback(() => {
    if (conversations.length === 0) {
      toast.error("æ²’æœ‰å°è©±è¨˜éŒ„å¯åŒ¯å‡º");
      return;
    }

    const content = conversations
      .map((msg) => {
        const speaker = msg.speaker === "nurse" ? "å°ç£äºº" : "å¤–åœ‹äºº";
        const time = msg.timestamp.toLocaleTimeString("zh-TW");
        return `[${time}] ${speaker}\nåŸæ–‡: ${msg.originalText}\nè­¯æ–‡: ${msg.translatedText}\n`;
      })
      .join("\n");

    const blob = new Blob([content], { type: "text/plain;charset=utf-8" });
    const url = URL.createObjectURL(blob);
    const a = document.createElement("a");
    a.href = url;
    a.download = `translation-${Date.now()}.txt`;
    a.click();
    URL.revokeObjectURL(url);

    toast.success("å°è©±è¨˜éŒ„å·²åŒ¯å‡º");
  }, [conversations]);

  useEffect(() => {
    return () => {
      if (processorRef.current) {
        processorRef.current.disconnect();
      }
      if (audioContextRef.current) {
        audioContextRef.current.close();
      }
      if (streamRef.current) {
        streamRef.current.getTracks().forEach((track) => track.stop());
      }
    };
  }, []);

  const getLanguageLabel = (code: string) => {
    const lang = LANGUAGE_OPTIONS.find((l) => l.value === code);
    return lang ? lang.label : code;
  };

  const getStatusMessage = () => {
    switch (processingStatus) {
      case "listening":
        return "ğŸŸ¢ ç­‰å¾…èªªè©±...";
      case "recognizing":
        return "ğŸŸ¡ æ­£åœ¨è¾¨è­˜èªéŸ³...";
      case "translating":
        return "ğŸŸ£ æ­£åœ¨ç¿»è­¯...";
      default:
        return "";
    }
  };

  return (
    <div className="min-h-screen flex flex-col bg-black/80 text-white">
      {/* Header */}
      <header className="p-6 flex items-center justify-between border-b border-white/10">
        <div className="flex items-center gap-4">
          <h1 className="text-2xl font-bold">å³æ™‚é›™å‘ç¿»è­¯ç³»çµ±</h1>
          <Select value={targetLanguage} onValueChange={setTargetLanguage} disabled={isRecording}>
            <SelectTrigger className="w-[180px] bg-white/10 border-white/20">
              <SelectValue placeholder="é¸æ“‡ç›®æ¨™èªè¨€" />
            </SelectTrigger>
            <SelectContent>
              {LANGUAGE_OPTIONS.map((lang) => (
                <SelectItem key={lang.value} value={lang.value}>
                  {lang.label}
                </SelectItem>
              ))}
            </SelectContent>
          </Select>
        </div>
        <div className="flex gap-2">
          <Button
            variant="outline"
            size="icon"
            onClick={exportConversations}
            disabled={conversations.length === 0}
            className="bg-white/10 border-white/20 hover:bg-white/20"
          >
            <Download className="h-4 w-4" />
          </Button>
          <Button
            variant="outline"
            size="icon"
            onClick={clearConversations}
            disabled={conversations.length === 0}
            className="bg-white/10 border-white/20 hover:bg-white/20"
          >
            <Trash2 className="h-4 w-4" />
          </Button>
        </div>
      </header>

      <div className="flex-1 p-6">
        <p className="text-center text-white/60 mb-4">
          é»æ“Šã€Œé–‹å§‹å°è©±ã€å¾Œï¼Œç³»çµ±å°‡æŒçºŒåµæ¸¬èªéŸ³ä¸¦å³æ™‚ç¿»è­¯
        </p>

        {/* è™•ç†ç‹€æ…‹æŒ‡ç¤ºå™¨ */}
        {processingStatus !== "idle" && (
          <div className="fixed top-20 right-6 bg-black/70 backdrop-blur-sm px-6 py-3 rounded-lg border border-white/20 text-lg">
            {getStatusMessage()}
          </div>
        )}

        {/* éŸ³é‡æŒ‡ç¤ºå™¨ */}
        {isRecording && (
          <div className="mb-6 flex items-center justify-center gap-4">
            <span className="text-sm text-white/60">éŸ³é‡:</span>
            <div className="w-64 h-2 bg-white/10 rounded-full overflow-hidden">
              <div
                className={`h-full transition-all duration-100 ${
                  speakingRef.current ? "bg-green-500" : "bg-white/30"
                }`}
                style={{ width: `${Math.min(audioLevel * 100, 100)}%` }}
              />
            </div>
            <span className="text-sm text-white/60">
              {speakingRef.current ? "åµæ¸¬åˆ°èªéŸ³" : "éœéŸ³"}
            </span>
          </div>
        )}

        {/* Conversation Display */}
        <div className="grid grid-cols-2 gap-6 mb-8">
          {/* å°ç£äºº (ä¸­æ–‡) */}
          <div>
            <h2 className="text-xl font-semibold mb-4 text-center">å°ç£äºº (ä¸­æ–‡)</h2>
            <div ref={nurseScrollRef} className="space-y-4 max-h-[500px] overflow-y-auto scroll-smooth">
              {conversations
                .filter((msg) => msg.speaker === "nurse")
                .map((msg) => (
                  <div
                    key={msg.id}
                    className="bg-white/10 backdrop-blur-sm p-4 rounded-lg border border-white/20"
                  >
                    <p className="text-lg mb-2">{msg.originalText}</p>
                    <p className="text-sm text-white/60">â†’ {msg.translatedText}</p>
                  </div>
                ))}
            </div>
          </div>

          {/* å¤–åœ‹äºº (å¤–èª) */}
          <div>
            <h2 className="text-xl font-semibold mb-4 text-center">å¤–åœ‹äºº (å¤–èª)</h2>
            <div ref={patientScrollRef} className="space-y-4 max-h-[500px] overflow-y-auto scroll-smooth">
              {conversations
                .filter((msg) => msg.speaker === "patient")
                .map((msg) => (
                  <div
                    key={msg.id}
                    className="bg-white/10 backdrop-blur-sm p-4 rounded-lg border border-white/20 relative"
                  >
                    <span className="absolute top-2 right-2 text-xs bg-white/20 px-2 py-1 rounded">
                      {getLanguageLabel(msg.detectedLanguage)}
                    </span>
                    <p className="text-lg mb-2">{msg.originalText}</p>
                    <p className="text-sm text-white/60">â†’ {msg.translatedText}</p>
                  </div>
                ))}
            </div>
          </div>
        </div>

        {/* Control Button */}
        <div className="flex justify-center">
          {!isRecording ? (
            <Button
              onClick={startRecording}
              size="lg"
              className="bg-green-600 hover:bg-green-700 text-white px-8 py-6 text-lg"
            >
              <Mic className="mr-2 h-5 w-5" />
              é–‹å§‹å°è©±
            </Button>
          ) : (
            <Button
              onClick={stopRecording}
              size="lg"
              variant="destructive"
              className="px-8 py-6 text-lg"
            >
              çµæŸå°è©±
            </Button>
          )}
        </div>
      </div>
    </div>
  );
}
