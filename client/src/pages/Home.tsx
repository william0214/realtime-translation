import { Button } from "@/components/ui/button";
import {
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
} from "@/components/ui/select";
import { trpc } from "@/lib/trpc";
import { Download, Mic, Trash2 } from "lucide-react";
import { useCallback, useEffect, useRef, useState } from "react";
import { toast } from "sonner";

type ConversationMessage = {
  id: number;
  speaker: "nurse" | "patient";
  originalText: string;
  translatedText: string;
  detectedLanguage: string;
  timestamp: Date;
};

type ProcessingStatus = "idle" | "listening" | "vad-detected" | "recognizing" | "translating" | "speaking";

const LANGUAGE_OPTIONS = [
  { value: "vi", label: "è¶Šå—èª" },
  { value: "id", label: "å°å°¼èª" },
  { value: "fil", label: "è²å¾‹è³“èª" },
  { value: "en", label: "è‹±æ–‡" },
  { value: "it", label: "ç¾©å¤§åˆ©èª" },
  { value: "ja", label: "æ—¥æ–‡" },
  { value: "ko", label: "éŸ“æ–‡" },
  { value: "th", label: "æ³°æ–‡" },
];

// VAD Settings
const RMS_THRESHOLD = 0.02; // Voice activity detection threshold
const SILENCE_DURATION_MS = 800; // 600-900ms range, using 800ms
const CHUNK_INTERVAL_MS = 100; // Small chunks for fine-grained VAD control

export default function Home() {
  const [isRecording, setIsRecording] = useState(false);
  const [conversations, setConversations] = useState<ConversationMessage[]>([]);
  const [targetLanguage, setTargetLanguage] = useState<string>("vi");
  const [audioLevel, setAudioLevel] = useState<number>(0);
  const [processingStatus, setProcessingStatus] = useState<ProcessingStatus>("idle");

  // Refs
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const messageIdRef = useRef(0);
  const nurseScrollRef = useRef<HTMLDivElement>(null);
  const patientScrollRef = useRef<HTMLDivElement>(null);

  // VAD-controlled chunk collection
  const currentChunksRef = useRef<Blob[]>([]); // Current speech segment chunks
  const lastSpeechTimeRef = useRef<number>(0);
  const isSpeakingRef = useRef<boolean>(false);
  const vadIntervalRef = useRef<number | null>(null);

  // tRPC mutations
  const translateMutation = trpc.translation.autoTranslate.useMutation();

  // Check audio level (VAD)
  const checkAudioLevel = useCallback(() => {
    if (!analyserRef.current) return false;

    const analyser = analyserRef.current;
    const dataArray = new Uint8Array(analyser.frequencyBinCount);
    analyser.getByteTimeDomainData(dataArray);

    // Calculate RMS
    let sum = 0;
    for (let i = 0; i < dataArray.length; i++) {
      const normalized = (dataArray[i] - 128) / 128;
      sum += normalized * normalized;
    }
    const rms = Math.sqrt(sum / dataArray.length);

    // Update audio level display
    setAudioLevel(rms / RMS_THRESHOLD);

    return rms > RMS_THRESHOLD;
  }, []);

  // Process speech segment (merge chunks and send to Whisper)
  const processSpeechSegment = useCallback(async () => {
    const chunks = currentChunksRef.current;
    if (chunks.length === 0) {
      console.log("[processSpeechSegment] No chunks to process");
      return;
    }

    console.log(`[processSpeechSegment] Processing ${chunks.length} chunks`);
    setProcessingStatus("recognizing");

    try {
      // Merge chunks using Blob (no ffmpeg needed!)
      const mergedBlob = new Blob(chunks, { type: "audio/webm" });
      console.log(`[processSpeechSegment] Merged blob size: ${mergedBlob.size} bytes`);

      // Convert to base64
      const reader = new FileReader();
      reader.onloadend = async () => {
        const base64Audio = (reader.result as string).split(",")[1];

        try {
          setProcessingStatus("translating");
          const result = await translateMutation.mutateAsync({
            audioBase64: base64Audio,
            filename: `audio-${Date.now()}.webm`,
            preferredTargetLang: targetLanguage,
          });

          if (result.success && result.sourceText && result.translatedText) {
            const speaker = result.direction === "nurse_to_patient" ? "nurse" : "patient";
            const newMessage: ConversationMessage = {
              id: messageIdRef.current++,
              speaker,
              originalText: result.sourceText,
              translatedText: result.translatedText,
              detectedLanguage: result.sourceLang || "unknown",
              timestamp: new Date(),
            };

            setConversations((prev) => [...prev, newMessage]);
            console.log(`[processSpeechSegment] Added message:`, newMessage);
          } else {
            console.error("[processSpeechSegment] Translation failed:", result.error);
            if (result.error && !result.error.includes("No speech detected")) {
              toast.error(result.error);
            }
          }
        } catch (error: any) {
          console.error("[processSpeechSegment] Error:", error);
          toast.error("è™•ç†èªéŸ³æ™‚ç™¼ç”ŸéŒ¯èª¤");
        } finally {
          setProcessingStatus("listening");
        }
      };
      reader.readAsDataURL(mergedBlob);

      // Clear chunks for next segment
      currentChunksRef.current = [];
    } catch (error: any) {
      console.error("[processSpeechSegment] Error:", error);
      toast.error("è™•ç†èªéŸ³æ™‚ç™¼ç”ŸéŒ¯èª¤");
      setProcessingStatus("listening");
    }
  }, [targetLanguage, translateMutation]);

  // Start VAD monitoring
  const startVADMonitoring = useCallback(() => {
    if (vadIntervalRef.current !== null) return;

    console.log("[VAD] Started VAD monitoring");
    setProcessingStatus("listening");

    vadIntervalRef.current = window.setInterval(() => {
      const isSpeaking = checkAudioLevel();
      const now = Date.now();

      if (isSpeaking) {
        // Speech detected
        lastSpeechTimeRef.current = now;
        if (!isSpeakingRef.current) {
          // Speech segment start (ç„¡è² â†’ æœ‰è²)
          isSpeakingRef.current = true;
          currentChunksRef.current = []; // Start new chunk buffer
          setProcessingStatus("vad-detected");
          console.log(`ğŸ”µ Speech started`);
        }
      } else {
        // Silence
        if (isSpeakingRef.current) {
          const silenceDuration = now - lastSpeechTimeRef.current;
          if (silenceDuration >= SILENCE_DURATION_MS) {
            // Speech segment end (æœ‰è² â†’ ç„¡è²ï¼ŒæŒçºŒ 800ms)
            isSpeakingRef.current = false;
            setProcessingStatus("listening");
            console.log(`ğŸŸ¢ Speech ended (silence: ${silenceDuration}ms), processing...`);

            // Process this speech segment
            processSpeechSegment();
          }
        }
      }
    }, 100);
  }, [checkAudioLevel, processSpeechSegment]);

  // Stop VAD monitoring
  const stopVADMonitoring = useCallback(() => {
    if (vadIntervalRef.current !== null) {
      window.clearInterval(vadIntervalRef.current);
      vadIntervalRef.current = null;
      console.log("[VAD] Stopped VAD monitoring");
    }
  }, []);

  // Start recording
  const startRecording = useCallback(async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        },
      });
      streamRef.current = stream;

      // Setup Web Audio API for VAD
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
      const source = audioContext.createMediaStreamSource(stream);
      const analyser = audioContext.createAnalyser();
      analyser.fftSize = 2048;
      source.connect(analyser);

      audioContextRef.current = audioContext;
      analyserRef.current = analyser;

      // Setup MediaRecorder (WebM Opus)
      const mimeType = "audio/webm;codecs=opus";
      if (!MediaRecorder.isTypeSupported(mimeType)) {
        throw new Error("WebM Opus format not supported");
      }

      const mediaRecorder = new MediaRecorder(stream, {
        mimeType,
        audioBitsPerSecond: 48000,
      });

      mediaRecorderRef.current = mediaRecorder;

      // Collect chunks only during speech
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0 && isSpeakingRef.current) {
          console.log(`[MediaRecorder] Chunk collected, size: ${event.data.size} bytes`);
          currentChunksRef.current.push(event.data);
        }
      };

      // Start recording with small chunk interval for VAD control
      mediaRecorder.start(CHUNK_INTERVAL_MS);
      console.log(`[MediaRecorder] Started recording with ${CHUNK_INTERVAL_MS}ms chunks`);

      // Start VAD monitoring
      startVADMonitoring();

      setIsRecording(true);
      toast.success("é–‹å§‹éŒ„éŸ³");
    } catch (error: any) {
      console.error("[startRecording] Error:", error);
      toast.error("ç„¡æ³•å•Ÿå‹•éº¥å…‹é¢¨");
    }
  }, [startVADMonitoring]);

  // Stop recording
  const stopRecording = useCallback(() => {
    console.log("[stopRecording] Stopping recording");
    stopVADMonitoring();

    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== "inactive") {
      mediaRecorderRef.current.stop();
      mediaRecorderRef.current = null;
    }

    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }

    if (streamRef.current) {
      streamRef.current.getTracks().forEach((track) => track.stop());
      streamRef.current = null;
    }

    // Clear chunks
    currentChunksRef.current = [];
    isSpeakingRef.current = false;
    setAudioLevel(0);
    setProcessingStatus("idle");
    setIsRecording(false);
    toast.success("åœæ­¢éŒ„éŸ³");
  }, [stopVADMonitoring]);

  // Toggle recording
  const toggleRecording = useCallback(() => {
    if (isRecording) {
      stopRecording();
    } else {
      startRecording();
    }
  }, [isRecording, startRecording, stopRecording]);

  // Clear conversations
  const clearConversations = useCallback(() => {
    setConversations([]);
    messageIdRef.current = 0;
    toast.success("å·²æ¸…é™¤å°è©±è¨˜éŒ„");
  }, []);

  // Export conversations
  const exportConversations = useCallback(() => {
    if (conversations.length === 0) {
      toast.error("æ²’æœ‰å°è©±è¨˜éŒ„å¯åŒ¯å‡º");
      return;
    }

    const content = conversations
      .map((msg) => {
        const speaker = msg.speaker === "nurse" ? "å°ç£äºº" : "å¤–åœ‹äºº";
        const time = msg.timestamp.toLocaleTimeString("zh-TW");
        return `[${time}] ${speaker}:\nåŸæ–‡: ${msg.originalText}\nè­¯æ–‡: ${msg.translatedText}\n`;
      })
      .join("\n");

    const blob = new Blob([content], { type: "text/plain;charset=utf-8" });
    const url = URL.createObjectURL(blob);
    const a = document.createElement("a");
    a.href = url;
    a.download = `å°è©±è¨˜éŒ„_${new Date().toISOString().slice(0, 10)}.txt`;
    a.click();
    URL.revokeObjectURL(url);

    toast.success("å°è©±è¨˜éŒ„å·²åŒ¯å‡º");
  }, [conversations]);

  // Auto-scroll to latest message
  useEffect(() => {
    if (nurseScrollRef.current) {
      nurseScrollRef.current.scrollTop = nurseScrollRef.current.scrollHeight;
    }
    if (patientScrollRef.current) {
      patientScrollRef.current.scrollTop = patientScrollRef.current.scrollHeight;
    }
  }, [conversations]);

  // Get status display
  const getStatusDisplay = () => {
    switch (processingStatus) {
      case "listening":
        return "ğŸ”µ ç­‰å¾…èªéŸ³...";
      case "vad-detected":
        return "ğŸŸ¢ åµæ¸¬åˆ°èªéŸ³...";
      case "recognizing":
        return "ğŸŸ¡ æ­£åœ¨è¾¨è­˜...";
      case "translating":
        return "ğŸŸ£ æ­£åœ¨ç¿»è­¯...";
      case "speaking":
        return "ğŸ”Š æ’­æ”¾ä¸­...";
      default:
        return "é–’ç½®";
    }
  };

  return (
    <div className="min-h-screen bg-black text-white flex flex-col">
      {/* Header */}
      <header className="border-b border-gray-800 p-4">
        <div className="container mx-auto flex items-center justify-between">
          <h1 className="text-2xl font-bold">å³æ™‚é›™å‘ç¿»è­¯ç³»çµ±</h1>
          <div className="flex items-center gap-4">
            <Select value={targetLanguage} onValueChange={setTargetLanguage} disabled={isRecording}>
              <SelectTrigger className="w-[180px] bg-gray-900 border-gray-700">
                <SelectValue placeholder="é¸æ“‡èªè¨€" />
              </SelectTrigger>
              <SelectContent className="bg-gray-900 border-gray-700">
                {LANGUAGE_OPTIONS.map((option) => (
                  <SelectItem key={option.value} value={option.value}>
                    {option.label}
                  </SelectItem>
                ))}
              </SelectContent>
            </Select>
            <Button variant="outline" size="icon" onClick={exportConversations} disabled={conversations.length === 0}>
              <Download className="h-4 w-4" />
            </Button>
            <Button variant="outline" size="icon" onClick={clearConversations} disabled={conversations.length === 0}>
              <Trash2 className="h-4 w-4" />
            </Button>
          </div>
        </div>
      </header>

      {/* Status Bar */}
      <div className="bg-gray-900 p-3 text-center text-sm">
        <div className="container mx-auto flex items-center justify-center gap-4">
          <span>{getStatusDisplay()}</span>
          {isRecording && (
            <div className="flex items-center gap-2">
              <span className="text-gray-400">éŸ³é‡:</span>
              <div className="w-32 h-2 bg-gray-700 rounded-full overflow-hidden">
                <div
                  className={`h-full transition-all ${audioLevel > 1 ? "bg-green-500" : "bg-gray-600"}`}
                  style={{ width: `${Math.min(audioLevel * 100, 100)}%` }}
                />
              </div>
            </div>
          )}
        </div>
      </div>

      {/* Main Content */}
      <main className="flex-1 container mx-auto p-6">
        <div className="text-center mb-6 text-gray-400">
          é»æ“Šã€Œé–‹å§‹å°è©±ã€å¾Œï¼Œç³»çµ±å°‡æŒçºŒåµæ¸¬èªéŸ³ä¸¦å³æ™‚ç¿»è­¯
        </div>

        <div className="grid grid-cols-2 gap-6 mb-6">
          {/* Nurse (Chinese) */}
          <div className="bg-gray-900 rounded-lg p-4">
            <h2 className="text-xl font-semibold mb-4 text-center">å°ç£äºº (ä¸­æ–‡)</h2>
            <div ref={nurseScrollRef} className="h-[400px] overflow-y-auto space-y-3">
              {conversations
                .filter((msg) => msg.speaker === "nurse")
                .map((msg) => (
                  <div key={msg.id} className="bg-gray-800 p-3 rounded">
                    <div className="text-sm text-gray-400 mb-1">{msg.timestamp.toLocaleTimeString("zh-TW")}</div>
                    <div className="font-medium mb-1">{msg.originalText}</div>
                    <div className="text-gray-400 text-sm">â†’ {msg.translatedText}</div>
                  </div>
                ))}
            </div>
          </div>

          {/* Patient (Foreign Language) */}
          <div className="bg-gray-900 rounded-lg p-4">
            <h2 className="text-xl font-semibold mb-4 text-center">å¤–åœ‹äºº (å¤–èª)</h2>
            <div ref={patientScrollRef} className="h-[400px] overflow-y-auto space-y-3">
              {conversations
                .filter((msg) => msg.speaker === "patient")
                .map((msg) => (
                  <div key={msg.id} className="bg-gray-800 p-3 rounded">
                    <div className="text-sm text-gray-400 mb-1">{msg.timestamp.toLocaleTimeString("zh-TW")}</div>
                    <div className="font-medium mb-1">{msg.originalText}</div>
                    <div className="text-gray-400 text-sm">â†’ {msg.translatedText}</div>
                  </div>
                ))}
            </div>
          </div>
        </div>

        {/* Control Button */}
        <div className="text-center">
          <Button
            size="lg"
            onClick={toggleRecording}
            className={`px-8 py-6 text-lg ${
              isRecording
                ? "bg-red-600 hover:bg-red-700"
                : "bg-green-600 hover:bg-green-700"
            }`}
          >
            <Mic className="mr-2 h-5 w-5" />
            {isRecording ? "çµæŸå°è©±" : "é–‹å§‹å°è©±"}
          </Button>
        </div>
      </main>
    </div>
  );
}
