import { useState, useRef, useCallback, useEffect } from "react";
import { Button } from "@/components/ui/button";
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from "@/components/ui/select";
import { toast } from "sonner";
import { trpc } from "@/lib/trpc";
import { getASRModeConfig } from "../../../shared/config";

// æ”¯æ´çš„èªè¨€åˆ—è¡¨
const SUPPORTED_LANGUAGES = [
  { code: "vi", name: "è¶Šå—èª", flag: "ğŸ‡»ğŸ‡³" },
  { code: "id", name: "å°å°¼èª", flag: "ğŸ‡®ğŸ‡©" },
  { code: "tl", name: "è²å¾‹è³“èª", flag: "ğŸ‡µğŸ‡­" },
  { code: "en", name: "è‹±æ–‡", flag: "ğŸ‡ºğŸ‡¸" },
  { code: "it", name: "ç¾©å¤§åˆ©èª", flag: "ğŸ‡®ğŸ‡¹" },
  { code: "ja", name: "æ—¥æ–‡", flag: "ğŸ‡¯ğŸ‡µ" },
  { code: "ko", name: "éŸ“æ–‡", flag: "ğŸ‡°ğŸ‡·" },
  { code: "th", name: "æ³°æ–‡", flag: "ğŸ‡¹ğŸ‡­" },
];

// è¨Šæ¯é¡å‹
interface Message {
  id: number;
  speaker: "nurse" | "patient";
  originalText: string;
  translatedText: string;
  timestamp: Date;
  status: "partial" | "final" | "translated";
}

// éº¥å…‹é¢¨è£ç½®é¡å‹
interface MicDevice {
  deviceId: string;
  label: string;
}

// éŸ³è¨Šè¨­å®š
const SAMPLE_RATE = 48000;

// å–®ä¸€éº¥å…‹é¢¨è™•ç†å™¨é¡å‹
interface MicProcessor {
  stream: MediaStream;
  audioContext: AudioContext;
  analyser: AnalyserNode;
  workletNode: AudioWorkletNode;
  sentenceBuffer: Float32Array[];
  isSpeaking: boolean;
  speechStartTime: number;
  silenceStartTime: number;
  lastPartialTime: number;
  sentenceEndTriggered: boolean;
  partialMessageId: number | null;
  vadInterval: number | null;
}

export default function TwoMic() {
  // ç‹€æ…‹ç®¡ç†
  const [targetLanguage, setTargetLanguage] = useState("vi");
  const [isRecording, setIsRecording] = useState(false);
  const [messages, setMessages] = useState<Message[]>([]);
  const [availableMics, setAvailableMics] = useState<MicDevice[]>([]);
  const [nurseMicId, setNurseMicId] = useState<string>("");
  const [patientMicId, setPatientMicId] = useState<string>("");
  const [nurseStatus, setNurseStatus] = useState<"idle" | "listening" | "recognizing" | "translating">("idle");
  const [patientStatus, setPatientStatus] = useState<"idle" | "listening" | "recognizing" | "translating">("idle");

  // Refs
  const nurseProcessorRef = useRef<MicProcessor | null>(null);
  const patientProcessorRef = useRef<MicProcessor | null>(null);
  const messageIdCounterRef = useRef(0);
  const messagesEndRef = useRef<HTMLDivElement>(null);

  // tRPC mutation
  const translateMutation = trpc.translation.autoTranslate.useMutation();

  // VAD é…ç½®
  const config = getASRModeConfig("normal");
  const RMS_THRESHOLD = config.rmsThreshold;
  const MIN_SPEECH_DURATION_MS = config.minSpeechDurationMs;
  const SILENCE_DURATION_MS = config.silenceDurationMs;
  const PARTIAL_CHUNK_INTERVAL_MS = config.partialChunkIntervalMs;

  // è‡ªå‹•æ»¾å‹•åˆ°æœ€æ–°è¨Šæ¯
  useEffect(() => {
    messagesEndRef.current?.scrollIntoView({ behavior: "smooth" });
  }, [messages]);

  // è¼‰å…¥å¯ç”¨éº¥å…‹é¢¨åˆ—è¡¨
  useEffect(() => {
    const loadMicrophones = async () => {
      try {
        // å…ˆè«‹æ±‚æ¬Šé™
        await navigator.mediaDevices.getUserMedia({ audio: true });
        
        const devices = await navigator.mediaDevices.enumerateDevices();
        const mics = devices
          .filter((d) => d.kind === "audioinput")
          .map((d, index) => ({
            deviceId: d.deviceId,
            label: d.label || `éº¥å…‹é¢¨ ${index + 1}`,
          }));
        
        setAvailableMics(mics);
        
        // é è¨­é¸æ“‡å‰å…©å€‹éº¥å…‹é¢¨
        if (mics.length >= 1 && !nurseMicId) {
          setNurseMicId(mics[0].deviceId);
        }
        if (mics.length >= 2 && !patientMicId) {
          setPatientMicId(mics[1].deviceId);
        }
      } catch (error) {
        console.error("Failed to load microphones:", error);
        toast.error("ç„¡æ³•å–å¾—éº¥å…‹é¢¨åˆ—è¡¨");
      }
    };

    loadMicrophones();

    // ç›£è½è£ç½®è®Šæ›´
    navigator.mediaDevices.addEventListener("devicechange", loadMicrophones);
    return () => {
      navigator.mediaDevices.removeEventListener("devicechange", loadMicrophones);
    };
  }, []);

  // WAV ç·¨ç¢¼å‡½æ•¸
  const encodeWAV = useCallback((samples: Float32Array, sampleRate: number): ArrayBuffer => {
    const buffer = new ArrayBuffer(44 + samples.length * 2);
    const view = new DataView(buffer);

    const writeString = (offset: number, string: string) => {
      for (let i = 0; i < string.length; i++) {
        view.setUint8(offset + i, string.charCodeAt(i));
      }
    };

    writeString(0, "RIFF");
    view.setUint32(4, 36 + samples.length * 2, true);
    writeString(8, "WAVE");
    writeString(12, "fmt ");
    view.setUint32(16, 16, true);
    view.setUint16(20, 1, true);
    view.setUint16(22, 1, true);
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, sampleRate * 2, true);
    view.setUint16(32, 2, true);
    view.setUint16(34, 16, true);
    writeString(36, "data");
    view.setUint32(40, samples.length * 2, true);

    let offset = 44;
    for (let i = 0; i < samples.length; i++) {
      const s = Math.max(-1, Math.min(1, samples[i]));
      view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7fff, true);
      offset += 2;
    }

    return buffer;
  }, []);

  // è™•ç† partial transcript
  const processPartialChunk = useCallback(async (
    pcmBuffer: Float32Array[],
    speaker: "nurse" | "patient",
    partialMessageId: number | null
  ) => {
    if (pcmBuffer.length < 12) return;

    const totalLength = pcmBuffer.reduce((acc, buf) => acc + buf.length, 0);
    const concatenated = new Float32Array(totalLength);
    let offset = 0;
    for (const buf of pcmBuffer) {
      concatenated.set(buf, offset);
      offset += buf.length;
    }

    const wavBuffer = encodeWAV(concatenated, SAMPLE_RATE);
    const blob = new Blob([wavBuffer], { type: "audio/wav" });

    const reader = new FileReader();
    reader.readAsDataURL(blob);
    reader.onloadend = async () => {
      const base64Audio = (reader.result as string).split(",")[1];

      try {
        const forceSourceLang = speaker === "nurse" ? "zh" : targetLanguage;
        
        const result = await translateMutation.mutateAsync({
          audioBase64: base64Audio,
          filename: `partial-${speaker}-${Date.now()}.wav`,
          preferredTargetLang: targetLanguage,
          forceSourceLang,
          forceSpeaker: speaker,
          transcriptOnly: true,
          asrMode: "normal",
        });

        if (result.success && result.sourceText && partialMessageId !== null) {
          setMessages((prev) =>
            prev.map((msg) =>
              msg.id === partialMessageId
                ? { ...msg, originalText: result.sourceText || "" }
                : msg
            )
          );
        }
      } catch (error) {
        console.error(`[Partial ${speaker}] Error:`, error);
      }
    };
  }, [targetLanguage, translateMutation, encodeWAV]);

  // è™•ç† final transcript
  const processFinalTranscript = useCallback(async (
    pcmBuffer: Float32Array[],
    speaker: "nurse" | "patient",
    partialMessageId: number | null,
    setStatus: (status: "idle" | "listening" | "recognizing" | "translating") => void
  ) => {
    if (pcmBuffer.length < 12) {
      setStatus("listening");
      return;
    }

    setStatus("recognizing");

    const MAX_FINAL_BUFFERS = 70;
    const finalBuffers = pcmBuffer.slice(-MAX_FINAL_BUFFERS);

    const totalLength = finalBuffers.reduce((acc, buf) => acc + buf.length, 0);
    const concatenated = new Float32Array(totalLength);
    let offset = 0;
    for (const buf of finalBuffers) {
      concatenated.set(buf, offset);
      offset += buf.length;
    }

    const wavBuffer = encodeWAV(concatenated, SAMPLE_RATE);
    const blob = new Blob([wavBuffer], { type: "audio/wav" });

    const reader = new FileReader();
    reader.readAsDataURL(blob);
    reader.onloadend = async () => {
      const base64Audio = (reader.result as string).split(",")[1];

      try {
        setStatus("translating");

        const forceSourceLang = speaker === "nurse" ? "zh" : targetLanguage;

        const result = await translateMutation.mutateAsync({
          audioBase64: base64Audio,
          filename: `final-${speaker}-${Date.now()}.wav`,
          preferredTargetLang: targetLanguage,
          forceSourceLang,
          forceSpeaker: speaker,
          transcriptOnly: false,
          asrMode: "normal",
        });

        if (result.success && result.sourceText) {
          if (partialMessageId !== null) {
            setMessages((prev) =>
              prev.map((msg) =>
                msg.id === partialMessageId
                  ? {
                      ...msg,
                      originalText: result.sourceText || "",
                      translatedText: result.translatedText || "",
                      status: "translated" as const,
                      timestamp: new Date(),
                    }
                  : msg
              )
            );
          } else {
            const newId = ++messageIdCounterRef.current;
            setMessages((prev) => [
              ...prev,
              {
                id: newId,
                speaker,
                originalText: result.sourceText || "",
                translatedText: result.translatedText || "",
                timestamp: new Date(),
                status: "translated",
              },
            ]);
          }
        } else {
          if (partialMessageId !== null) {
            setMessages((prev) => prev.filter((msg) => msg.id !== partialMessageId));
          }
          if (result.error && !result.error.includes("No speech detected")) {
            toast.error(`âŒ ç¿»è­¯å¤±æ•—: ${result.error}`);
          }
        }
      } catch (error) {
        console.error(`[Final ${speaker}] Error:`, error);
        toast.error("ç¿»è­¯ç™¼ç”ŸéŒ¯èª¤");
        if (partialMessageId !== null) {
          setMessages((prev) => prev.filter((msg) => msg.id !== partialMessageId));
        }
      } finally {
        setStatus("listening");
      }
    };
  }, [targetLanguage, translateMutation, encodeWAV]);

  // å»ºç«‹å–®ä¸€éº¥å…‹é¢¨è™•ç†å™¨
  const createMicProcessor = useCallback(async (
    deviceId: string,
    speaker: "nurse" | "patient",
    setStatus: (status: "idle" | "listening" | "recognizing" | "translating") => void
  ): Promise<MicProcessor | null> => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: { deviceId: { exact: deviceId } },
      });

      const audioContext = new AudioContext({ sampleRate: SAMPLE_RATE });
      const analyser = audioContext.createAnalyser();
      analyser.fftSize = 2048;

      const source = audioContext.createMediaStreamSource(stream);
      source.connect(analyser);

      await audioContext.audioWorklet.addModule("/audio-processor.js");
      const workletNode = new AudioWorkletNode(audioContext, "audio-processor");

      const processor: MicProcessor = {
        stream,
        audioContext,
        analyser,
        workletNode,
        sentenceBuffer: [],
        isSpeaking: false,
        speechStartTime: 0,
        silenceStartTime: 0,
        lastPartialTime: 0,
        sentenceEndTriggered: false,
        partialMessageId: null,
        vadInterval: null,
      };

      // è™•ç†éŸ³è¨Šè³‡æ–™
      workletNode.port.onmessage = (event) => {
        if (processor.isSpeaking) {
          processor.sentenceBuffer.push(new Float32Array(event.data));
        }
      };

      source.connect(workletNode);
      workletNode.connect(audioContext.destination);

      // VAD ç›£æ§
      const checkAudioLevel = () => {
        const dataArray = new Uint8Array(analyser.frequencyBinCount);
        analyser.getByteTimeDomainData(dataArray);

        let sum = 0;
        for (let i = 0; i < dataArray.length; i++) {
          const normalized = (dataArray[i] - 128) / 128;
          sum += normalized * normalized;
        }
        const rms = Math.sqrt(sum / dataArray.length);

        const now = Date.now();
        const isSpeaking = rms > RMS_THRESHOLD;

        if (isSpeaking) {
          if (!processor.isSpeaking) {
            processor.isSpeaking = true;
            processor.speechStartTime = now;
            processor.sentenceEndTriggered = false;
            processor.sentenceBuffer = [];

            const newId = ++messageIdCounterRef.current;
            processor.partialMessageId = newId;
            setMessages((prev) => [
              ...prev,
              {
                id: newId,
                speaker,
                originalText: "",
                translatedText: "",
                timestamp: new Date(),
                status: "partial",
              },
            ]);

            console.log(`ğŸ”µ [${speaker}] Speech started`);
          }
          processor.silenceStartTime = 0;

          if (now - processor.lastPartialTime >= PARTIAL_CHUNK_INTERVAL_MS) {
            const recentBuffers = processor.sentenceBuffer.slice(-50);
            if (recentBuffers.length >= 12) {
              processPartialChunk(recentBuffers, speaker, processor.partialMessageId);
              processor.lastPartialTime = now;
            }
          }
        } else {
          if (processor.isSpeaking) {
            if (processor.silenceStartTime === 0) {
              processor.silenceStartTime = now;
            }

            const silenceDuration = now - processor.silenceStartTime;
            const speechDuration = now - processor.speechStartTime;

            if (silenceDuration >= SILENCE_DURATION_MS && !processor.sentenceEndTriggered) {
              console.log(`ğŸŸ¢ [${speaker}] Speech ended (duration: ${speechDuration}ms)`);

              processor.sentenceEndTriggered = true;
              processor.isSpeaking = false;

              if (speechDuration >= MIN_SPEECH_DURATION_MS) {
                const finalBuffers = [...processor.sentenceBuffer];
                const partialId = processor.partialMessageId;
                processor.sentenceBuffer = [];
                processor.lastPartialTime = 0;
                processor.partialMessageId = null;
                processFinalTranscript(finalBuffers, speaker, partialId, setStatus);
              } else {
                console.log(`âš ï¸ [${speaker}] Speech too short, discarding`);
                if (processor.partialMessageId !== null) {
                  setMessages((prev) => prev.filter((msg) => msg.id !== processor.partialMessageId));
                  processor.partialMessageId = null;
                }
                processor.sentenceBuffer = [];
                processor.lastPartialTime = 0;
              }
            }
          }
        }
      };

      processor.vadInterval = window.setInterval(checkAudioLevel, 50);

      return processor;
    } catch (error) {
      console.error(`Failed to create mic processor for ${speaker}:`, error);
      toast.error(`ç„¡æ³•å•Ÿå‹•${speaker === "nurse" ? "å°ç£äºº" : "å¤–åœ‹äºº"}éº¥å…‹é¢¨`);
      return null;
    }
  }, [RMS_THRESHOLD, MIN_SPEECH_DURATION_MS, SILENCE_DURATION_MS, PARTIAL_CHUNK_INTERVAL_MS, processPartialChunk, processFinalTranscript]);

  // åœæ­¢å–®ä¸€éº¥å…‹é¢¨è™•ç†å™¨
  const stopMicProcessor = useCallback((processor: MicProcessor | null) => {
    if (!processor) return;

    if (processor.vadInterval) {
      clearInterval(processor.vadInterval);
    }
    if (processor.workletNode) {
      processor.workletNode.disconnect();
    }
    if (processor.audioContext) {
      processor.audioContext.close();
    }
    if (processor.stream) {
      processor.stream.getTracks().forEach((track) => track.stop());
    }
  }, []);

  // é–‹å§‹éŒ„éŸ³
  const startRecording = async () => {
    if (!nurseMicId || !patientMicId) {
      toast.error("è«‹å…ˆé¸æ“‡å…©å€‹éº¥å…‹é¢¨");
      return;
    }

    if (nurseMicId === patientMicId) {
      toast.error("è«‹é¸æ“‡ä¸åŒçš„éº¥å…‹é¢¨");
      return;
    }

    // å•Ÿå‹•å°ç£äººéº¥å…‹é¢¨
    const nurseProcessor = await createMicProcessor(nurseMicId, "nurse", setNurseStatus);
    if (!nurseProcessor) return;
    nurseProcessorRef.current = nurseProcessor;

    // å•Ÿå‹•å¤–åœ‹äººéº¥å…‹é¢¨
    const patientProcessor = await createMicProcessor(patientMicId, "patient", setPatientStatus);
    if (!patientProcessor) {
      stopMicProcessor(nurseProcessor);
      return;
    }
    patientProcessorRef.current = patientProcessor;

    setIsRecording(true);
    setNurseStatus("listening");
    setPatientStatus("listening");
    toast.success("é›™éº¥å…‹é¢¨å·²å•Ÿå‹•");
  };

  // åœæ­¢éŒ„éŸ³
  const stopRecording = () => {
    stopMicProcessor(nurseProcessorRef.current);
    stopMicProcessor(patientProcessorRef.current);
    nurseProcessorRef.current = null;
    patientProcessorRef.current = null;

    // æ¸…é™¤ partial è¨Šæ¯
    setMessages((prev) => prev.filter((msg) => msg.status !== "partial"));

    setIsRecording(false);
    setNurseStatus("idle");
    setPatientStatus("idle");
    toast.info("å·²åœæ­¢éŒ„éŸ³");
  };

  // æ¸…é™¤å°è©±
  const clearMessages = () => {
    setMessages([]);
    messageIdCounterRef.current = 0;
  };

  // å–å¾—ç›®æ¨™èªè¨€è³‡è¨Š
  const getTargetLanguageInfo = () => {
    return SUPPORTED_LANGUAGES.find((l) => l.code === targetLanguage) || SUPPORTED_LANGUAGES[0];
  };

  // ç‹€æ…‹æŒ‡ç¤ºå™¨å…ƒä»¶
  const StatusIndicator = ({ status, label }: { status: string; label: string }) => (
    <div className={`flex items-center gap-2 px-3 py-1.5 rounded-full text-sm ${
      status === "idle" ? "bg-gray-700" :
      status === "listening" ? "bg-green-700" :
      status === "recognizing" ? "bg-yellow-700" :
      "bg-blue-700"
    }`}>
      <span className={`w-2 h-2 rounded-full ${
        status === "idle" ? "bg-gray-400" :
        status === "listening" ? "bg-green-400 animate-pulse" :
        status === "recognizing" ? "bg-yellow-400 animate-pulse" :
        "bg-blue-400 animate-pulse"
      }`} />
      <span>{label}</span>
    </div>
  );

  // éæ¿¾è¨Šæ¯
  const nurseMessages = messages.filter((m) => m.speaker === "nurse");
  const patientMessages = messages.filter((m) => m.speaker === "patient");

  return (
    <div className="min-h-screen bg-gradient-to-br from-gray-900 via-gray-800 to-gray-900 text-white">
      {/* Header */}
      <header className="bg-black/30 backdrop-blur-sm border-b border-gray-700/50 p-4">
        <div className="container mx-auto">
          <div className="flex flex-col md:flex-row md:items-center md:justify-between gap-4">
            <h1 className="text-xl md:text-2xl font-bold">ğŸ¤ğŸ¤ é›™éº¥å…‹é¢¨ç¿»è­¯</h1>
            
            <div className="flex flex-wrap items-center gap-4">
              {/* èªè¨€é¸æ“‡ */}
              <div className="flex items-center gap-2">
                <span className="text-sm text-gray-400">å¤–èªï¼š</span>
                <Select
                  value={targetLanguage}
                  onValueChange={setTargetLanguage}
                  disabled={isRecording}
                >
                  <SelectTrigger className="w-[130px] bg-gray-800 border-gray-600">
                    <SelectValue />
                  </SelectTrigger>
                  <SelectContent>
                    {SUPPORTED_LANGUAGES.map((lang) => (
                      <SelectItem key={lang.code} value={lang.code}>
                        {lang.flag} {lang.name}
                      </SelectItem>
                    ))}
                  </SelectContent>
                </Select>
              </div>
            </div>
          </div>

          {/* éº¥å…‹é¢¨é¸æ“‡ */}
          <div className="mt-4 grid grid-cols-1 md:grid-cols-2 gap-4">
            <div className="flex items-center gap-2">
              <span className="text-sm text-gray-400 whitespace-nowrap">ğŸ‡¹ğŸ‡¼ å°ç£äººéº¥å…‹é¢¨ï¼š</span>
              <Select
                value={nurseMicId}
                onValueChange={setNurseMicId}
                disabled={isRecording}
              >
                <SelectTrigger className="flex-1 bg-gray-800 border-gray-600">
                  <SelectValue placeholder="é¸æ“‡éº¥å…‹é¢¨" />
                </SelectTrigger>
                <SelectContent>
                  {availableMics.map((mic) => (
                    <SelectItem key={mic.deviceId} value={mic.deviceId}>
                      {mic.label}
                    </SelectItem>
                  ))}
                </SelectContent>
              </Select>
            </div>

            <div className="flex items-center gap-2">
              <span className="text-sm text-gray-400 whitespace-nowrap">{getTargetLanguageInfo().flag} å¤–åœ‹äººéº¥å…‹é¢¨ï¼š</span>
              <Select
                value={patientMicId}
                onValueChange={setPatientMicId}
                disabled={isRecording}
              >
                <SelectTrigger className="flex-1 bg-gray-800 border-gray-600">
                  <SelectValue placeholder="é¸æ“‡éº¥å…‹é¢¨" />
                </SelectTrigger>
                <SelectContent>
                  {availableMics.map((mic) => (
                    <SelectItem key={mic.deviceId} value={mic.deviceId}>
                      {mic.label}
                    </SelectItem>
                  ))}
                </SelectContent>
              </Select>
            </div>
          </div>
        </div>
      </header>

      {/* Main Content */}
      <main className="container mx-auto p-4 md:p-6">
        {/* ç‹€æ…‹æŒ‡ç¤ºå™¨ */}
        <div className="flex items-center justify-center gap-4 mb-6 flex-wrap">
          <StatusIndicator 
            status={nurseStatus} 
            label={`ğŸ‡¹ğŸ‡¼ ${nurseStatus === "idle" ? "å¾…æ©Ÿ" : nurseStatus === "listening" ? "è†è½ä¸­" : nurseStatus === "recognizing" ? "è­˜åˆ¥ä¸­" : "ç¿»è­¯ä¸­"}`} 
          />
          <StatusIndicator 
            status={patientStatus} 
            label={`${getTargetLanguageInfo().flag} ${patientStatus === "idle" ? "å¾…æ©Ÿ" : patientStatus === "listening" ? "è†è½ä¸­" : patientStatus === "recognizing" ? "è­˜åˆ¥ä¸­" : "ç¿»è­¯ä¸­"}`} 
          />
        </div>

        {/* é›™æ¬„å°è©±æ¡† */}
        <div className="grid grid-cols-1 md:grid-cols-2 gap-4 mb-6">
          {/* å°ç£äººå°è©±æ¡† */}
          <div className="bg-gray-800/50 backdrop-blur-sm rounded-2xl border border-gray-700/50 p-4">
            <div className="flex items-center justify-between mb-4">
              <h2 className="text-lg font-semibold">ğŸ‡¹ğŸ‡¼ å°ç£äººï¼ˆä¸­æ–‡ï¼‰</h2>
              {nurseMessages.length > 0 && (
                <span className="text-xs text-gray-500">{nurseMessages.length} å‰‡</span>
              )}
            </div>

            <div className="h-[300px] md:h-[400px] overflow-y-auto space-y-3 pr-2">
              {nurseMessages.length === 0 ? (
                <div className="flex items-center justify-center h-full text-gray-500 text-sm">
                  èªªä¸­æ–‡ â†’ ç¿»è­¯æˆ{getTargetLanguageInfo().name}
                </div>
              ) : (
                nurseMessages.map((msg) => (
                  <div
                    key={msg.id}
                    className={`p-3 rounded-xl ${
                      msg.status === "partial"
                        ? "bg-yellow-900/30 border border-yellow-700/50"
                        : "bg-blue-900/30 border border-blue-700/50"
                    }`}
                  >
                    {msg.status === "partial" ? (
                      <div className="text-gray-300 italic">
                        {msg.originalText || "åµæ¸¬ä¸­..."}
                      </div>
                    ) : (
                      <>
                        <div className="text-white mb-2">{msg.originalText}</div>
                        <div className="border-t border-gray-600/50 pt-2">
                          <div className="text-cyan-400">{msg.translatedText}</div>
                        </div>
                        <div className="text-xs text-gray-500 mt-2">
                          {msg.timestamp.toLocaleTimeString()}
                        </div>
                      </>
                    )}
                  </div>
                ))
              )}
            </div>
          </div>

          {/* å¤–åœ‹äººå°è©±æ¡† */}
          <div className="bg-gray-800/50 backdrop-blur-sm rounded-2xl border border-gray-700/50 p-4">
            <div className="flex items-center justify-between mb-4">
              <h2 className="text-lg font-semibold">{getTargetLanguageInfo().flag} å¤–åœ‹äººï¼ˆ{getTargetLanguageInfo().name}ï¼‰</h2>
              {patientMessages.length > 0 && (
                <span className="text-xs text-gray-500">{patientMessages.length} å‰‡</span>
              )}
            </div>

            <div className="h-[300px] md:h-[400px] overflow-y-auto space-y-3 pr-2">
              {patientMessages.length === 0 ? (
                <div className="flex items-center justify-center h-full text-gray-500 text-sm">
                  èªª{getTargetLanguageInfo().name} â†’ ç¿»è­¯æˆä¸­æ–‡
                </div>
              ) : (
                patientMessages.map((msg) => (
                  <div
                    key={msg.id}
                    className={`p-3 rounded-xl ${
                      msg.status === "partial"
                        ? "bg-yellow-900/30 border border-yellow-700/50"
                        : "bg-green-900/30 border border-green-700/50"
                    }`}
                  >
                    {msg.status === "partial" ? (
                      <div className="text-gray-300 italic">
                        {msg.originalText || "åµæ¸¬ä¸­..."}
                      </div>
                    ) : (
                      <>
                        <div className="text-white mb-2">{msg.originalText}</div>
                        <div className="border-t border-gray-600/50 pt-2">
                          <div className="text-cyan-400">{msg.translatedText}</div>
                        </div>
                        <div className="text-xs text-gray-500 mt-2">
                          {msg.timestamp.toLocaleTimeString()}
                        </div>
                      </>
                    )}
                  </div>
                ))
              )}
            </div>
          </div>
        </div>

        {/* æ§åˆ¶æŒ‰éˆ• */}
        <div className="flex justify-center gap-4">
          <Button
            size="lg"
            onClick={isRecording ? stopRecording : startRecording}
            disabled={!nurseMicId || !patientMicId}
            className={`px-8 py-6 text-lg rounded-full ${
              isRecording
                ? "bg-red-600 hover:bg-red-700"
                : "bg-green-600 hover:bg-green-700"
            }`}
          >
            {isRecording ? "â¹ï¸ çµæŸå°è©±" : "ğŸ¤ é–‹å§‹å°è©±"}
          </Button>

          {messages.length > 0 && (
            <Button
              size="lg"
              variant="outline"
              onClick={clearMessages}
              className="px-6 py-6 text-lg rounded-full"
            >
              ğŸ—‘ï¸ æ¸…é™¤
            </Button>
          )}
        </div>

        {/* æç¤º */}
        {!isRecording && availableMics.length < 2 && (
          <div className="mt-6 text-center text-yellow-400 text-sm">
            âš ï¸ åµæ¸¬åˆ° {availableMics.length} å€‹éº¥å…‹é¢¨ï¼Œå»ºè­°é€£æ¥ 2 å€‹éº¥å…‹é¢¨ä»¥ä½¿ç”¨é›™éº¥å…‹é¢¨åŠŸèƒ½
          </div>
        )}

        <div ref={messagesEndRef} />
      </main>

      {/* Footer */}
      <footer className="fixed bottom-0 left-0 right-0 bg-black/30 backdrop-blur-sm border-t border-gray-700/50 p-2">
        <div className="container mx-auto text-center text-xs text-gray-500">
          å°ç£äººèªªä¸­æ–‡ â†’ {getTargetLanguageInfo().name} ï½œ å¤–åœ‹äººèªª{getTargetLanguageInfo().name} â†’ ä¸­æ–‡
        </div>
      </footer>
    </div>
  );
}
