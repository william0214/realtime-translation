# 效能分析與改善建議

## 目前架構分析

### 處理流程

```
使用者說話
  ↓
[1] 麥克風錄音 (AudioWorklet, 48kHz)
  ↓
[2] VAD 檢測 (RMS 計算, 100ms 間隔)
  ↓
[3] PCM 緩衝區累積
  ↓
[4] 轉換為 WebM 格式
  ↓
[5] Base64 編碼
  ↓
[6] 發送到後端 API
  ↓
[7] Whisper 語音識別 (OpenAI API)
  ↓
[8] LLM 翻譯 (OpenAI API)
  ↓
[9] 返回前端顯示
```

### 時間分佈估計

根據典型場景（1-2 秒語音）：

| 步驟 | 時間 | 佔比 | 可優化 |
|------|------|------|--------|
| 1. 錄音 | ~1000ms | 25% | ❌ (使用者說話時間) |
| 2. VAD 檢測 | ~100ms | 2.5% | ✅ (已優化) |
| 3. 緩衝區處理 | ~50ms | 1.25% | ⚠️ (可優化) |
| 4. WebM 轉換 | ~100ms | 2.5% | ⚠️ (可優化) |
| 5. Base64 編碼 | ~50ms | 1.25% | ⚠️ (可優化) |
| 6. 網路傳輸 | ~200ms | 5% | ⚠️ (取決於網路) |
| 7. Whisper API | ~1500ms | 37.5% | ⚠️ (API 速度) |
| 8. 翻譯 API | ~1000ms | 25% | ⚠️ (API 速度) |
| 9. 顯示 | ~10ms | 0.25% | ✅ (已優化) |
| **總計** | **~4010ms** | **100%** | |

## 已完成的優化

### 1. VAD 參數優化 ✅

**優化內容**：
- 提高 RMS 閾值（0.04 → 0.08）
- 延長靜音檢測時間（800ms → 1000ms）
- 加入最小語音長度檢測（300ms）

**效果**：
- ✅ 減少誤判，避免處理無效音訊
- ✅ 降低 API 呼叫次數
- ✅ 節省 Whisper API 成本

### 2. 時間日誌系統 ✅

**優化內容**：
- 記錄收到音訊時間
- 記錄 Whisper 開始/完成時間
- 記錄翻譯開始/完成時間
- 計算各步驟耗時

**效果**：
- ✅ 可精確定位效能瓶頸
- ✅ 方便比較不同後端效能

### 3. 雙後端支援 ✅

**優化內容**：
- Node.js 後端（tRPC）
- Go 後端（REST API）
- 一鍵切換比較效能

**效果**：
- ✅ 可測試不同實作的速度
- ✅ 選擇最佳方案

## 效能瓶頸分析

### 主要瓶頸：API 呼叫時間（62.5%）

**Whisper API（37.5%）**：
- 音訊長度：1-2 秒
- 預估耗時：1500-2500ms
- **瓶頸原因**：OpenAI API 處理時間

**翻譯 API（25%）**：
- 文字長度：10-50 字
- 預估耗時：1000-1500ms
- **瓶頸原因**：LLM 推理時間

### 次要瓶頸：音訊處理（7.5%）

**WebM 轉換 + Base64 編碼**：
- 預估耗時：150ms
- **瓶頸原因**：瀏覽器 MediaRecorder 編碼

## 改善建議

### 🔥 高優先級（立即可實施）

#### 1. 使用 Whisper Streaming API（如果可用）

**目前**：等待完整音訊 → 一次性識別
**改善**：邊錄邊識別 → 即時返回結果

**預期效果**：
- Whisper 延遲：1500ms → 500ms（減少 66%）
- 總延遲：4000ms → 3000ms（減少 25%）

**實作難度**：中（需要 API 支援）

#### 2. 平行處理 Whisper 和翻譯（部分場景）

**目前**：Whisper → 翻譯（串行）
**改善**：Whisper 識別中文時，直接翻譯（不需等待完整識別）

**預期效果**：
- 總延遲：4000ms → 3500ms（減少 12.5%）

**實作難度**：低

**實作方式**：
```typescript
// 目前：串行
const whisperResult = await callWhisper(audio);
const translation = await callTranslation(whisperResult.text);

// 改善：平行（如果 Whisper 支援 streaming）
const whisperStream = callWhisperStreaming(audio);
whisperStream.on('partial', (partialText) => {
  // 立即開始翻譯部分結果
  callTranslation(partialText);
});
```

#### 3. 音訊格式優化

**目前**：WebM（瀏覽器預設）
**改善**：直接發送 PCM / WAV（減少編碼時間）

**預期效果**：
- WebM 轉換：100ms → 0ms
- Base64 編碼：50ms → 30ms
- 總節省：120ms

**實作難度**：低

**實作方式**：
```typescript
// 直接使用 PCM buffer，不經過 MediaRecorder
const pcmBuffer = concatenatePCMBuffers(sentenceBufferRef.current);
const wavBlob = createWAVBlob(pcmBuffer, SAMPLE_RATE);
const base64Audio = await blobToBase64(wavBlob);
```

### ⚠️ 中優先級（需要測試）

#### 4. 調整音訊採樣率

**目前**：48kHz
**改善**：16kHz（Whisper 推薦）

**預期效果**：
- 音訊大小：減少 66%
- 網路傳輸：200ms → 70ms（減少 65%）
- Whisper 處理：可能略快

**風險**：音質下降，識別準確度可能降低

**實作難度**：低

#### 5. 使用 WebSocket 取代 HTTP

**目前**：每次翻譯發送一次 HTTP 請求
**改善**：建立 WebSocket 連線，持續傳輸

**預期效果**：
- 減少 HTTP 握手時間
- 支援雙向即時通訊
- 總延遲：可能減少 100-200ms

**實作難度**：中（需要修改後端）

### 🔮 低優先級（長期優化）

#### 6. 使用本地 Whisper 模型

**目前**：呼叫 OpenAI API
**改善**：使用 Whisper.cpp 或 Transformers.js 在本地執行

**預期效果**：
- Whisper 延遲：1500ms → 500ms（取決於硬體）
- 無網路延遲
- 無 API 成本

**風險**：
- 需要強大的客戶端硬體（GPU）
- 模型載入時間長
- 準確度可能略低

**實作難度**：高

#### 7. 使用更快的翻譯模型

**目前**：GPT-4 / GPT-3.5
**改善**：使用專門的翻譯模型（例如：NLLB, M2M100）

**預期效果**：
- 翻譯延遲：1000ms → 300ms（減少 70%）

**風險**：
- 翻譯品質可能下降
- 需要自建推理服務

**實作難度**：高

## 測試計畫

### 階段 1：VAD 優化驗證（已完成）

- [x] 提高 RMS 閾值
- [x] 加入最小語音長度檢測
- [ ] 測試開車環境是否解決 AAAAA 問題

### 階段 2：音訊處理優化

- [ ] 實作直接 PCM → WAV 轉換
- [ ] 測試 16kHz 採樣率效果
- [ ] 比較處理時間差異

### 階段 3：API 呼叫優化

- [ ] 測試 Go 後端 vs Node.js 後端速度
- [ ] 記錄 Whisper 和翻譯的實際耗時
- [ ] 找出最慢的環節

### 階段 4：架構優化（如果需要）

- [ ] 評估 Whisper Streaming API 可行性
- [ ] 評估 WebSocket 架構改造成本
- [ ] 評估本地 Whisper 模型可行性

## 效能目標

### 目前效能（估計）

- 總延遲：~4000ms
- Whisper：~1500ms
- 翻譯：~1000ms

### 短期目標（1-2 週）

- 總延遲：< 3500ms（減少 12.5%）
- 透過音訊處理優化和平行處理達成

### 中期目標（1-2 個月）

- 總延遲：< 3000ms（減少 25%）
- 透過 Whisper Streaming 或 WebSocket 達成

### 長期目標（3-6 個月）

- 總延遲：< 2000ms（減少 50%）
- 透過本地 Whisper 模型或專用翻譯模型達成

## 監控指標

### 關鍵指標（已實作）

1. **總耗時**：從收到音訊到顯示翻譯
2. **Whisper 耗時**：語音識別時間
3. **翻譯耗時**：文字翻譯時間

### 建議新增指標

1. **音訊處理時間**：PCM → WebM → Base64
2. **網路傳輸時間**：發送請求到收到回應
3. **VAD 誤判率**：短促噪音被過濾的次數
4. **API 失敗率**：Whisper 或翻譯失敗的比例

## 成本分析

### API 成本（OpenAI）

**Whisper API**：
- 價格：$0.006 / 分鐘
- 平均音訊長度：1.5 秒
- 單次成本：$0.00015

**GPT-4 翻譯**：
- 價格：$0.03 / 1K tokens（輸入）+ $0.06 / 1K tokens（輸出）
- 平均 tokens：50（輸入）+ 50（輸出）
- 單次成本：$0.006

**總成本**：~$0.00615 / 次翻譯

### 優化後成本

**使用本地 Whisper**：
- Whisper 成本：$0
- 翻譯成本：$0.006
- 總成本：$0.006 / 次（減少 2.4%）

**使用本地翻譯模型**：
- Whisper 成本：$0.00015
- 翻譯成本：$0
- 總成本：$0.00015 / 次（減少 97.6%）

## 結論

### 立即可實施的優化

1. ✅ VAD 參數優化（已完成）
2. 🔄 音訊格式優化（PCM → WAV）
3. 🔄 測試 Go 後端效能
4. 🔄 調整採樣率（48kHz → 16kHz）

### 需要進一步評估

1. Whisper Streaming API（取決於 API 支援）
2. WebSocket 架構（需要後端改造）
3. 本地 Whisper 模型（需要硬體支援）

### 建議優先順序

1. **第一步**：測試目前 VAD 優化效果，確認 AAAAA 問題是否解決
2. **第二步**：實作音訊格式優化，減少處理時間
3. **第三步**：測試 Go 後端，比較兩種實作的速度
4. **第四步**：根據測試結果，決定是否需要更大規模的架構改造
